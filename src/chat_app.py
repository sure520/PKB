import streamlit as st
import os
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch, RunnablePassthrough
from vector_store import VectorStore
from zhipuai_embedding import ZhipuAIEmbeddings
from deepseek_llm import DeepSeekChat
from search_manager import SearchManager
from datetime import datetime, timedelta

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def get_search_manager():
    """è·å–æœç´¢ç®¡ç†å™¨"""
    embedding = ZhipuAIEmbeddings()
    vector_store = VectorStore(
        embedding=embedding,
        persist_directory="../vector_db"
    )
    vector_store.load_existing()
    return SearchManager(vector_store)

def get_qa_history_chain(search_manager):
    """æ„å»ºé—®ç­”é“¾"""
    llm = DeepSeekChat(
        model="deepseek-chat",
        temperature=0.7,
        max_tokens=2000
    )
    
    # é—®é¢˜æ€»ç»“æ¨¡æ¿
    condense_question_system_template = (
        "è¯·æ ¹æ®èŠå¤©è®°å½•æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼Œ"
        "å¦‚æœæ²¡æœ‰å¤šä½™çš„èŠå¤©è®°å½•åˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚"
    )
    condense_question_prompt = ChatPromptTemplate([
        ("system", condense_question_system_template),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ])

    # æ„å»ºæ£€ç´¢åˆ†æ”¯
    retrieve_docs = RunnableBranch(
        (
            lambda x: not x.get("chat_history"), 
            lambda x: search_manager.advanced_search(x["input"])
        ),
        (
            lambda x: True,
            RunnablePassthrough.assign(
            condensed_query=condense_question_prompt 
            | llm 
            | StrOutputParser()
        ) | (lambda x: search_manager.advanced_search(x["condensed_query"]))
        )
    )

    # å›ç­”ç”Ÿæˆæ¨¡æ¿
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªåŸºäºçŸ¥è¯†åº“çš„é—®ç­”åŠ©æ‰‹ã€‚ "
        "è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ "
        "å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚ "
        "è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚"
        "\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ])

    # æ„å»ºé—®ç­”é“¾
    qa_chain = (
        RunnablePassthrough().assign(context=lambda x: "\n\n".join(doc.page_content for doc in x["context"]))
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    return RunnablePassthrough().assign(
        context=retrieve_docs,
    ).assign(answer=qa_chain)

def gen_response(chain, input, chat_history):
    """ç”Ÿæˆå›ç­”"""
    response = chain.stream({
        "input": input,
        "chat_history": chat_history
    })
    full_response = ""
    for res in response:
        if "answer" in res.keys():
            yield res["answer"]
            full_response += res["answer"]  # æ‹¼æ¥å®Œæ•´å›ç­”
    return full_response  # è¿”å›å®Œæ•´å›ç­”

def main():
    """ä¸»å‡½æ•°"""
    st.set_page_config(
        page_title="ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹",
        page_icon="ğŸ“š",
        layout="wide"
    )
    
    st.title("ğŸ“š ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹")
    st.markdown("""
    è¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”æ‚¨å…³äºçŸ¥è¯†åº“å†…å®¹çš„é—®é¢˜ã€‚
    è¯·åœ¨ä¸‹æ–¹çš„è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚
    """)

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "search_manager" not in st.session_state:
        st.session_state.search_manager = get_search_manager()
    if "qa_history_chain" not in st.session_state:
        st.session_state.qa_history_chain = get_qa_history_chain(st.session_state.search_manager)

    # åˆ›å»ºä¾§è¾¹æ 
    with st.sidebar:
        st.header("æœç´¢è®¾ç½®")
        
        # ç›¸ä¼¼åº¦é˜ˆå€¼è®¾ç½®
        score_threshold = st.slider(
            "ç›¸ä¼¼åº¦é˜ˆå€¼",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.1
        )
        
        # æ—¥æœŸèŒƒå›´è¿‡æ»¤
        st.subheader("æ—¥æœŸèŒƒå›´")
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input(
                "å¼€å§‹æ—¥æœŸ",
                value=datetime.now() - timedelta(days=30)
            )
        with col2:
            end_date = st.date_input(
                "ç»“æŸæ—¥æœŸ",
                value=datetime.now()
            )
            
        # æœç´¢å†å²
        st.subheader("æœç´¢å†å²")
        history = st.session_state.search_manager.get_search_history()
        for item in history:
            with st.expander(f"{item['query']} ({item['timestamp'].strftime('%Y-%m-%d %H:%M')})"):
                st.write(f"ç»“æœæ•°é‡: {item['result_count']}")
                if item['filters']:
                    st.write("è¿‡æ»¤æ¡ä»¶:", item['filters'])
                    
        if st.button("æ¸…ç©ºæœç´¢å†å²"):
            st.session_state.search_manager.clear_search_history()
            st.success("æœç´¢å†å²å·²æ¸…ç©ºï¼")  # æ·»åŠ æç¤ºä¿¡æ¯

    # å°†ç›¸ä¼¼åº¦é˜ˆå€¼å’Œæ—¥æœŸèŒƒå›´ä¼ é€’ç»™æœç´¢ç®¡ç†å™¨
    filters = {
        "score_threshold": score_threshold,
        "date_range": (start_date, end_date)
    }
    st.session_state.search_manager.set_filters(filters)  # å‡è®¾ SearchManager æ”¯æŒè®¾ç½®è¿‡æ»¤æ¡ä»¶

    # åˆ›å»ºä¸»ç•Œé¢
    messages = st.container(height=550)
    
    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with messages.chat_message(message[0]):
            st.write(message[1])

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append(("human", prompt))
        with messages.chat_message("human"):
            st.write(prompt)

        # è·å–ç›¸ä¼¼çš„å†å²æŸ¥è¯¢
        similar_queries = st.session_state.search_manager.get_similar_queries(prompt)
        if similar_queries:
            with st.expander("ç›¸ä¼¼çš„å†å²æŸ¥è¯¢"):
                for query in similar_queries:
                    st.write(f"- {query}")

        # ç”Ÿæˆå›ç­”
        with messages.chat_message("assistant"):
            answer_generator = gen_response(
                chain=st.session_state.qa_history_chain,
                input=prompt,
                chat_history=st.session_state.messages
            )
            output = "".join(st.write_stream(answer_generator))  # æ‹¼æ¥å®Œæ•´å›ç­”
        
        # ä¿å­˜åŠ©æ‰‹å›ç­”
        st.session_state.messages.append(("assistant", output))

if __name__ == "__main__":
    main()
